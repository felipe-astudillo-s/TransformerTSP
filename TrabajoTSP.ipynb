{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e36a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. IMPORTS\n",
    "# ==========================================\n",
    "\n",
    "#probando si esto se commitea Segunda prueba desde otro pc\n",
    "#Nota de intiti: si van a trabajar desde un entorno local (Visual), \n",
    "# aseg√∫rense de tener instaladas las librer√≠as necesarias.\n",
    "#tutorial: ctrl + √± para abrir el terminal y luego pegar los siguientes comandos:\n",
    "#comando para instalar torch: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -> En caso que quieran usar GPU.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests \n",
    "import gc # Garbage Collector para gesti√≥n de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "73e8365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU DETECTADA: NVIDIA GeForce RTX 5060\n",
      "   (Memoria disponible: 8.55 GB)\n",
      "\n",
      "‚öôÔ∏è Sincronizando con GitHub (felipe-astudillo-s/TransformerTSP)...\n",
      "üîç Consultando API para: Data/Easy...\n",
      "‚ùå Error 404: No existe la carpeta 'Data/Easy' en la rama 'main'.\n",
      "üîç Consultando API para: Data/Medium...\n",
      "‚úÖ Fase Data/Medium: 20 archivos listos en d:\\Proyectos\\Transformer_TSP\\TransformerTSP\\data_repo\\MEDIUM\n",
      "üîç Consultando API para: Data/Hard...\n",
      "‚ùå Error 404: No existe la carpeta 'Data/Hard' en la rama 'main'.\n",
      "\n",
      "üìÇ Rutas configuradas correctamente.\n",
      "üöÄ Listo para ejecutar el Bloque de Entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURACI√ìN, GPU Y DESCARGA DE DATOS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# --- A. CONFIGURACI√ìN DEL HARDWARE (DEVICE) ---\n",
    "# Esto es vital para que el Bloque de entrenamiento sepa qu√© usar\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ GPU DETECTADA: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   (Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è GPU NO DETECTADA: Entrenando en CPU (ser√° lento).\")\n",
    "\n",
    "# --- B. CONFIGURACI√ìN DEL REPOSITORIO ---\n",
    "REPO_USER = \"felipe-astudillo-s\"\n",
    "REPO_NAME = \"TransformerTSP\"\n",
    "BRANCH = \"main\" # ‚ö†Ô∏è IMPORTANTE: Si tus datos no est√°n en 'main', cambia esto por el nombre de tu rama o commit.\n",
    "\n",
    "REPO_FOLDERS = {\n",
    "    \"EASY\":   \"Data/Easy\",\n",
    "    \"MEDIUM\": \"Data/Medium\",\n",
    "    \"HARD\":   \"Data/Hard\"\n",
    "}\n",
    "\n",
    "BASE_LOCAL_DIR = os.path.join(os.getcwd(), \"data_repo\")\n",
    "\n",
    "def download_folder_from_github(user, repo, repo_folder_path, local_output_dir, branch=\"main\"):\n",
    "    \"\"\"Descarga todos los .npz de una carpeta de GitHub usando la API.\"\"\"\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{repo_folder_path}?ref={branch}\"\n",
    "    \n",
    "    print(f\"üîç Consultando API para: {repo_folder_path}...\")\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 404:\n",
    "            print(f\"‚ùå Error 404: No existe la carpeta '{repo_folder_path}' en la rama '{branch}'.\")\n",
    "            return local_output_dir\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error API ({response.status_code}): {response.text}\")\n",
    "            return local_output_dir\n",
    "\n",
    "        files_list = response.json()\n",
    "        \n",
    "        if not os.path.exists(local_output_dir):\n",
    "            os.makedirs(local_output_dir)\n",
    "\n",
    "        if isinstance(files_list, dict) and 'message' in files_list:\n",
    "            print(\"‚ùå Error: La ruta parece no ser una carpeta v√°lida.\")\n",
    "            return local_output_dir\n",
    "\n",
    "        count = 0\n",
    "        for item in files_list:\n",
    "            if item['type'] == 'file' and item['name'].endswith('.npz'):\n",
    "                local_path = os.path.join(local_output_dir, item['name'])\n",
    "                if not os.path.exists(local_path):\n",
    "                    try:\n",
    "                        r = requests.get(item['download_url'])\n",
    "                        with open(local_path, 'wb') as f:\n",
    "                            f.write(r.content)\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Fall√≥ {item['name']}: {e}\")\n",
    "                else:\n",
    "                    count += 1 # Ya exist√≠a\n",
    "        \n",
    "        print(f\"‚úÖ Fase {repo_folder_path}: {count} archivos listos en {local_output_dir}\")\n",
    "        return local_output_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error de conexi√≥n: {e}\")\n",
    "        return local_output_dir\n",
    "\n",
    "# --- C. EJECUCI√ìN DE DESCARGA ---\n",
    "PATHS = {}\n",
    "print(f\"\\n‚öôÔ∏è Sincronizando con GitHub ({REPO_USER}/{REPO_NAME})...\")\n",
    "\n",
    "for phase_name, repo_path in REPO_FOLDERS.items():\n",
    "    local_target = os.path.join(BASE_LOCAL_DIR, phase_name)\n",
    "    final_path = download_folder_from_github(REPO_USER, REPO_NAME, repo_path, local_target, BRANCH)\n",
    "    PATHS[phase_name] = final_path\n",
    "\n",
    "# --- D. CURRICULUM ---\n",
    "CURRICULUM = [\n",
    "    {\"phase\": \"EASY\",   \"epochs\": 20, \"lr\": 1e-3, \"bs\": 128},\n",
    "    {\"phase\": \"MEDIUM\", \"epochs\": 15, \"lr\": 1e-4, \"bs\": 64},\n",
    "    {\"phase\": \"HARD\",   \"epochs\": 30, \"lr\": 1e-4, \"bs\": 32}\n",
    "]\n",
    "\n",
    "print(f\"\\nüìÇ Rutas configuradas correctamente.\")\n",
    "print(f\"üöÄ Listo para ejecutar el Bloque de Entrenamiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "be3ed76f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 2. ARQUITECTURA DEL MODELO (POINTER NETWORK)\n",
    "# ==========================================\n",
    "\n",
    "# ENCODER (Sin Positional Encoding y con return memory)\n",
    "class Encoder(nn.Module):\n",
    "    def __init__(self, input_dim, d_model=128, nhead=8, num_layers=3, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        self.input_proj = nn.Linear(input_dim, d_model)\n",
    "\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, src_key_padding_mask=None):\n",
    "        # x: [batch, seq_len, input_dim]\n",
    "        h = self.input_proj(x)  # [B, S, d_model]\n",
    "\n",
    "        memory = self.encoder(h, src_key_padding_mask=src_key_padding_mask)\n",
    "\n",
    "        return memory\n",
    "\n",
    "\n",
    "# --- 2. DECODER\n",
    "class PointerDecoder(nn.Module):\n",
    "    def __init__(self, d_model=128, nhead=8, num_layers=2, dropout=0.1, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        self.start_token = nn.Parameter(torch.randn(1, 1, d_model))\n",
    "        self.step_emb = nn.Embedding(max_seq_len, d_model)\n",
    "\n",
    "        decoder_layer = nn.TransformerDecoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=d_model*4,\n",
    "            dropout=dropout,\n",
    "            batch_first=True\n",
    "        )\n",
    "        self.decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "        self.query_proj = nn.Linear(d_model, d_model)\n",
    "\n",
    "    def forward(self, memory, tgt_indices=None, mask_visited=None, teacher_forcing=True):\n",
    "        B, S, d = memory.size()\n",
    "        device = memory.device\n",
    "        max_T = tgt_indices.size(1) if (tgt_indices is not None and teacher_forcing) else S\n",
    "\n",
    "        start = self.start_token.expand(B, -1, -1)\n",
    "        logits_steps = []\n",
    "        decoder_inputs = start\n",
    "        current_mask = torch.zeros(B, S, dtype=torch.bool).to(device)\n",
    "\n",
    "        for t in range(max_T):\n",
    "            step_emb = self.step_emb(torch.tensor([t], device=device)).unsqueeze(0).expand(B, -1, -1)\n",
    "            dec_in = decoder_inputs + step_emb\n",
    "            dec_out = self.decoder(dec_in, memory, memory_key_padding_mask=None)\n",
    "\n",
    "            q_t = dec_out[:, -1, :]\n",
    "            q = self.query_proj(q_t).unsqueeze(1)\n",
    "\n",
    "            scores = torch.matmul(q, memory.transpose(1,2)) / math.sqrt(d)\n",
    "            scores = scores.squeeze(1)\n",
    "\n",
    "            if not teacher_forcing:\n",
    "                scores = scores.masked_fill(current_mask, float('-inf'))\n",
    "\n",
    "            logits_steps.append(scores)\n",
    "\n",
    "            if teacher_forcing and tgt_indices is not None:\n",
    "                idx_t = tgt_indices[:, t]\n",
    "            else:\n",
    "                probs = F.softmax(scores, dim=-1)\n",
    "                idx_t = probs.argmax(dim=-1)\n",
    "                new_visit = F.one_hot(idx_t, num_classes=S).bool()\n",
    "                current_mask = current_mask | new_visit\n",
    "\n",
    "            next_emb = torch.gather(memory, 1, idx_t.view(B,1,1).expand(-1,1,d)).squeeze(1).unsqueeze(1)\n",
    "            decoder_inputs = torch.cat([decoder_inputs, next_emb], dim=1)\n",
    "\n",
    "        return torch.stack(logits_steps, dim=1)\n",
    "\n",
    "\n",
    "#  MODELO PRINCIPAL\n",
    "class EncoderPointerModel(nn.Module):\n",
    "    def __init__(self, input_dim=2, d_model=128, enc_layers=3, dec_layers=2, nhead=8, max_seq_len=128):\n",
    "        super().__init__()\n",
    "        # CORRECCI√ìN: Encoder ya no recibe max_seq_len\n",
    "        self.encoder = Encoder(\n",
    "            input_dim=input_dim,\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=enc_layers\n",
    "        )\n",
    "        self.decoder = PointerDecoder(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            num_layers=dec_layers,\n",
    "            max_seq_len=max_seq_len\n",
    "        )\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, tgt_indices=None, mask_padding=None,\n",
    "                mask_visited=None, teacher_forcing=True,\n",
    "                return_probabilities=False):\n",
    "\n",
    "        memory = self.encoder(x, src_key_padding_mask=mask_padding)\n",
    "\n",
    "        logits = self.decoder(\n",
    "            memory,\n",
    "            tgt_indices=tgt_indices,\n",
    "            mask_visited=mask_visited,\n",
    "            teacher_forcing=teacher_forcing\n",
    "        )\n",
    "\n",
    "        if return_probabilities:\n",
    "            return torch.softmax(logits, dim=-1)\n",
    "        return logits"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "653e886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. UTILIDADES DE EVALUACI√ìN\n",
    "# ==========================================\n",
    "def calculate_gap(model, loader, device):\n",
    "    \"\"\"Calcula el Optimality GAP (%) usando Greedy Decoding en un batch.\"\"\"\n",
    "    model.eval()\n",
    "    try:\n",
    "        # Tomamos solo el primer batch para no demorar el entrenamiento\n",
    "        batch_x, batch_y = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        return 0.0 # Loader vac√≠o\n",
    "\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    batch_size, n_nodes, _ = batch_x.size()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Inferencia Greedy (Teacher Forcing = False)\n",
    "        # El modelo genera la secuencia de √≠ndices autom√°ticamente\n",
    "        logits = model(batch_x, teacher_forcing=False)\n",
    "        # logits: [Batch, N, N_nodes]\n",
    "\n",
    "        pred_indices = logits.argmax(dim=2) # [Batch, N]\n",
    "\n",
    "        # Stackear para formar tour\n",
    "        pred_tour = pred_indices\n",
    "\n",
    "    # --- C√°lculo de Distancias ---\n",
    "    def get_dist(pts, idx):\n",
    "        # pts: [B, N, 2], idx: [B, N]\n",
    "        gathered = torch.gather(pts, 1, idx.unsqueeze(-1).expand(-1, -1, 2))\n",
    "        next_pts = torch.roll(gathered, -1, dims=1)\n",
    "        return torch.norm(gathered - next_pts, dim=2).sum(dim=1)\n",
    "\n",
    "    cost_model = get_dist(batch_x, pred_tour)\n",
    "    cost_oracle = get_dist(batch_x, batch_y)\n",
    "\n",
    "    gap = ((cost_model - cost_oracle) / cost_oracle).mean().item() * 100\n",
    "    return gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "34631666",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ INICIANDO ENTRENAMIENTO SOTA (Multi-Archivo)\n",
      "\n",
      "============================================================\n",
      "üéì FASE ACTUAL: EASY | Epochs: 20\n",
      "============================================================\n",
      "‚ö†Ô∏è ALERTA: No encontr√© datos en d:\\Proyectos\\Transformer_TSP\\TransformerTSP\\data_repo\\EASY. Saltando fase.\n",
      "\n",
      "============================================================\n",
      "üéì FASE ACTUAL: MEDIUM | Epochs: 15\n",
      "============================================================\n",
      "üìÇ Archivos detectados: 20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Epoca 1 Terminada | Loss: 2.3547 | üìä GAP: 27.01%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    üìâ Epoca 2 Terminada | Loss: 1.9000 | üìä GAP: 28.11%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                       \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 60\u001b[39m\n\u001b[32m     57\u001b[39m optimizer.zero_grad()\n\u001b[32m     59\u001b[39m \u001b[38;5;66;03m# Teacher Forcing: Pasamos la soluci√≥n completa (batch_y) como target\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m logits = \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbatch_x\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtgt_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mbatch_y\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     61\u001b[39m \u001b[38;5;66;03m# logits: [Batch, Seq, N_ciudades]\u001b[39;00m\n\u001b[32m     62\u001b[39m \n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Aplanar para Loss\u001b[39;00m\n\u001b[32m     64\u001b[39m \u001b[38;5;66;03m# logits.reshape(-1, logits.size(-1)) -> [Batch*Seq, N_ciudades]\u001b[39;00m\n\u001b[32m     65\u001b[39m \u001b[38;5;66;03m# batch_y.reshape(-1) -> [Batch*Seq]\u001b[39;00m\n\u001b[32m     66\u001b[39m loss = criterion(logits.reshape(-\u001b[32m1\u001b[39m, logits.size(-\u001b[32m1\u001b[39m)), batch_y.reshape(-\u001b[32m1\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 112\u001b[39m, in \u001b[36mEncoderPointerModel.forward\u001b[39m\u001b[34m(self, x, tgt_indices, mask_padding, mask_visited, teacher_forcing, return_probabilities)\u001b[39m\n\u001b[32m    106\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x, tgt_indices=\u001b[38;5;28;01mNone\u001b[39;00m, mask_padding=\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[32m    107\u001b[39m             mask_visited=\u001b[38;5;28;01mNone\u001b[39;00m, teacher_forcing=\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[32m    108\u001b[39m             return_probabilities=\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[32m    110\u001b[39m     memory = \u001b[38;5;28mself\u001b[39m.encoder(x, src_key_padding_mask=mask_padding)\n\u001b[32m--> \u001b[39m\u001b[32m112\u001b[39m     logits = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    113\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    114\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_indices\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    115\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmask_visited\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmask_visited\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    116\u001b[39m \u001b[43m        \u001b[49m\u001b[43mteacher_forcing\u001b[49m\u001b[43m=\u001b[49m\u001b[43mteacher_forcing\u001b[49m\n\u001b[32m    117\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    119\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m return_probabilities:\n\u001b[32m    120\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m torch.softmax(logits, dim=-\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 60\u001b[39m, in \u001b[36mPointerDecoder.forward\u001b[39m\u001b[34m(self, memory, tgt_indices, mask_visited, teacher_forcing)\u001b[39m\n\u001b[32m     58\u001b[39m step_emb = \u001b[38;5;28mself\u001b[39m.step_emb(torch.tensor([t], device=device)).unsqueeze(\u001b[32m0\u001b[39m).expand(B, -\u001b[32m1\u001b[39m, -\u001b[32m1\u001b[39m)\n\u001b[32m     59\u001b[39m dec_in = decoder_inputs + step_emb\n\u001b[32m---> \u001b[39m\u001b[32m60\u001b[39m dec_out = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdec_in\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m     62\u001b[39m q_t = dec_out[:, -\u001b[32m1\u001b[39m, :]\n\u001b[32m     63\u001b[39m q = \u001b[38;5;28mself\u001b[39m.query_proj(q_t).unsqueeze(\u001b[32m1\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:628\u001b[39m, in \u001b[36mTransformerDecoder.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m    625\u001b[39m tgt_is_causal = _detect_is_causal_mask(tgt_mask, tgt_is_causal, seq_len)\n\u001b[32m    627\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m mod \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m.layers:\n\u001b[32m--> \u001b[39m\u001b[32m628\u001b[39m     output = \u001b[43mmod\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    629\u001b[39m \u001b[43m        \u001b[49m\u001b[43moutput\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    631\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    632\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    633\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    634\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_key_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    635\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtgt_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    636\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmemory_is_causal\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    637\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    639\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.norm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    640\u001b[39m     output = \u001b[38;5;28mself\u001b[39m.norm(output)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1775\u001b[39m, in \u001b[36mModule._wrapped_call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1773\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._compiled_call_impl(*args, **kwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[32m   1774\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1775\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1786\u001b[39m, in \u001b[36mModule._call_impl\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   1781\u001b[39m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[32m   1782\u001b[39m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[32m   1783\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m._backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m._forward_pre_hooks\n\u001b[32m   1784\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[32m   1785\u001b[39m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[32m-> \u001b[39m\u001b[32m1786\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1788\u001b[39m result = \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m   1789\u001b[39m called_always_called_hooks = \u001b[38;5;28mset\u001b[39m()\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1131\u001b[39m, in \u001b[36mTransformerDecoderLayer.forward\u001b[39m\u001b[34m(self, tgt, memory, tgt_mask, memory_mask, tgt_key_padding_mask, memory_key_padding_mask, tgt_is_causal, memory_is_causal)\u001b[39m\n\u001b[32m   1122\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm1(\n\u001b[32m   1123\u001b[39m         x + \u001b[38;5;28mself\u001b[39m._sa_block(x, tgt_mask, tgt_key_padding_mask, tgt_is_causal)\n\u001b[32m   1124\u001b[39m     )\n\u001b[32m   1125\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm2(\n\u001b[32m   1126\u001b[39m         x\n\u001b[32m   1127\u001b[39m         + \u001b[38;5;28mself\u001b[39m._mha_block(\n\u001b[32m   1128\u001b[39m             x, memory, memory_mask, memory_key_padding_mask, memory_is_causal\n\u001b[32m   1129\u001b[39m         )\n\u001b[32m   1130\u001b[39m     )\n\u001b[32m-> \u001b[39m\u001b[32m1131\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.norm3(x + \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_ff_block\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m   1133\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m x\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\modules\\transformer.py:1176\u001b[39m, in \u001b[36mTransformerDecoderLayer._ff_block\u001b[39m\u001b[34m(self, x)\u001b[39m\n\u001b[32m   1175\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_ff_block\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) -> Tensor:\n\u001b[32m-> \u001b[39m\u001b[32m1176\u001b[39m     x = \u001b[38;5;28mself\u001b[39m.linear2(\u001b[38;5;28mself\u001b[39m.dropout(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mactivation\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mlinear1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m))\n\u001b[32m   1177\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.dropout3(x)\n",
      "\u001b[36mFile \u001b[39m\u001b[32md:\\Proyectos\\Transformer_TSP\\venv\\Lib\\site-packages\\torch\\nn\\functional.py:1697\u001b[39m, in \u001b[36mrelu\u001b[39m\u001b[34m(input, inplace)\u001b[39m\n\u001b[32m   1695\u001b[39m     result = torch.relu_(\u001b[38;5;28minput\u001b[39m)\n\u001b[32m   1696\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1697\u001b[39m     result = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrelu\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m result\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 4. BUCLE DE ENTRENAMIENTO (LAZY LOADING)\n",
    "# ==========================================\n",
    "\n",
    "# Instanciar Modelo con la nueva clase\n",
    "model = EncoderPointerModel(input_dim=2, d_model=128, nhead=8, enc_layers=3, dec_layers=2, max_seq_len=150).to(DEVICE) # Ajusta max_seq_len seg√∫n tus datos m√°s grandes\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(\"\\nüöÄ INICIANDO ENTRENAMIENTO SOTA (Multi-Archivo)\")\n",
    "\n",
    "for stage in CURRICULUM:\n",
    "    phase = stage['phase']\n",
    "    folder_path = PATHS[phase]\n",
    "\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üéì FASE ACTUAL: {phase} | Epochs: {stage['epochs']}\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # Buscar archivos .npz y .tpz\n",
    "    all_files = glob.glob(os.path.join(folder_path, \"*.npz\")) + \\\n",
    "                glob.glob(os.path.join(folder_path, \"*.tpz\"))\n",
    "\n",
    "    if not all_files:\n",
    "        print(f\"‚ö†Ô∏è ALERTA: No encontr√© datos en {folder_path}. Saltando fase.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"üìÇ Archivos detectados: {len(all_files)}\")\n",
    "\n",
    "    optimizer = optim.Adam(model.parameters(), lr=stage['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    for epoch in range(stage['epochs']):\n",
    "        model.train()\n",
    "        epoch_loss_accum = 0\n",
    "        total_batches = 0\n",
    "        current_gap = 0\n",
    "\n",
    "        # --- BUCLE SOBRE ARCHIVOS (Lazy Loading) ---\n",
    "        for file_idx, file_path in enumerate(all_files):\n",
    "            try:\n",
    "                # 1. Cargar Archivo a RAM\n",
    "                data = np.load(file_path)\n",
    "                points = torch.FloatTensor(data['points'])\n",
    "                solutions = torch.LongTensor(data['solutions'])\n",
    "\n",
    "                # Normalizaci√≥n defensiva\n",
    "                if points.max() > 1.0: points /= points.max()\n",
    "\n",
    "                dataset = TensorDataset(points, solutions)\n",
    "                loader = DataLoader(dataset, batch_size=stage['bs'], shuffle=True)\n",
    "\n",
    "                # 2. Entrenar sobre este archivo\n",
    "                pbar = tqdm(loader, desc=f\"Ep {epoch+1} | {os.path.basename(file_path)}\", leave=False)\n",
    "\n",
    "                for batch_x, batch_y in pbar:\n",
    "                    batch_x, batch_y = batch_x.to(DEVICE), batch_y.to(DEVICE)\n",
    "                    optimizer.zero_grad()\n",
    "\n",
    "                    # Teacher Forcing: Pasamos la soluci√≥n completa (batch_y) como target\n",
    "                    logits = model(batch_x, tgt_indices=batch_y, teacher_forcing=True)\n",
    "                    # logits: [Batch, Seq, N_ciudades]\n",
    "\n",
    "                    # Aplanar para Loss\n",
    "                    # logits.reshape(-1, logits.size(-1)) -> [Batch*Seq, N_ciudades]\n",
    "                    # batch_y.reshape(-1) -> [Batch*Seq]\n",
    "                    loss = criterion(logits.reshape(-1, logits.size(-1)), batch_y.reshape(-1))\n",
    "\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "\n",
    "                    epoch_loss_accum += loss.item()\n",
    "                    pbar.set_postfix({'loss': loss.item()})\n",
    "\n",
    "                total_batches += len(loader)\n",
    "\n",
    "                # Calcular GAP solo en el √∫ltimo archivo de la √©poca para ahorrar tiempo\n",
    "                if file_idx == len(all_files) - 1:\n",
    "                    current_gap = calculate_gap(model, loader, DEVICE)\n",
    "\n",
    "                # 3. LIMPIEZA DE MEMORIA\n",
    "                del data, points, solutions, dataset, loader\n",
    "                gc.collect()\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"‚ùå Error leyendo archivo {file_path}: {e}\")\n",
    "                continue\n",
    "\n",
    "        # --- REPORTE DE √âPOCA ---\n",
    "        avg_loss = epoch_loss_accum / total_batches if total_batches > 0 else 0\n",
    "        print(f\"    üìâ Epoca {epoch+1} Terminada | Loss: {avg_loss:.4f} | üìä GAP: {current_gap:.2f}%\")\n",
    "\n",
    "        # Scheduler Step\n",
    "        scheduler.step(avg_loss)\n",
    "\n",
    "        # Guardar Checkpoint\n",
    "        save_file = os.path.join(folder_path, f\"checkpoint_{phase}_best.pth\")\n",
    "        torch.save(model.state_dict(), save_file)\n",
    "\n",
    "print(\"\\nüèÜ ENTRENAMIENTO COMPLETADO EXITOSAMENTE.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
