{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7e36a4b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 0. IMPORTS\n",
    "# ==========================================\n",
    "\n",
    "#probando si esto se commitea Segunda prueba desde otro pc\n",
    "#Nota de intiti: si van a trabajar desde un entorno local (Visual), \n",
    "# aseg√∫rense de tener instaladas las librer√≠as necesarias.\n",
    "#tutorial: ctrl + √± para abrir el terminal y luego pegar los siguientes comandos:\n",
    "#comando para instalar torch: pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121 -> En caso que quieran usar GPU.\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "from ortools.constraint_solver import pywrapcp, routing_enums_pb2\n",
    "import concurrent.futures # LIBRER√çA MAGICA PARA PARALELISMO\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import requests \n",
    "import gc # Garbage Collector para gesti√≥n de memoria"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "73e8365d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ GPU DETECTADA: NVIDIA GeForce RTX 3050 Laptop GPU\n",
      "   (Memoria disponible: 4.29 GB)\n",
      "\n",
      "‚öôÔ∏è Sincronizando con GitHub (felipe-astudillo-s/TransformerTSP)...\n",
      "üîç Consultando API para: Data/Easy...\n",
      "‚úÖ Fase Data/Easy: 20 archivos listos en d:\\VISUAL\\gith\\TransformerTSP\\data_repo\\EASY\n",
      "üîç Consultando API para: Data/Medium...\n",
      "‚úÖ Fase Data/Medium: 20 archivos listos en d:\\VISUAL\\gith\\TransformerTSP\\data_repo\\MEDIUM\n",
      "üîç Consultando API para: Data/Hard...\n",
      "‚úÖ Fase Data/Hard: 10 archivos listos en d:\\VISUAL\\gith\\TransformerTSP\\data_repo\\HARD\n",
      "\n",
      "üìÇ Rutas configuradas correctamente.\n",
      "üöÄ Listo para ejecutar el Bloque de Entrenamiento.\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 1. CONFIGURACI√ìN, GPU Y DESCARGA DE DATOS\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# --- A. CONFIGURACI√ìN DEL HARDWARE (DEVICE) ---\n",
    "# Esto es vital para que el Bloque de entrenamiento sepa qu√© usar\n",
    "if torch.cuda.is_available():\n",
    "    DEVICE = torch.device(\"cuda\")\n",
    "    print(f\"‚úÖ GPU DETECTADA: {torch.cuda.get_device_name(0)}\")\n",
    "    print(f\"   (Memoria disponible: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB)\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")\n",
    "    print(\"‚ö†Ô∏è GPU NO DETECTADA: Entrenando en CPU (ser√° lento).\")\n",
    "\n",
    "# --- B. CONFIGURACI√ìN DEL REPOSITORIO ---\n",
    "REPO_USER = \"felipe-astudillo-s\"\n",
    "REPO_NAME = \"TransformerTSP\"\n",
    "BRANCH = \"main\" # ‚ö†Ô∏è IMPORTANTE: Si tus datos no est√°n en 'main', cambia esto por el nombre de tu rama o commit.\n",
    "\n",
    "REPO_FOLDERS = {\n",
    "    \"EASY\":   \"Data/Easy\",\n",
    "    \"MEDIUM\": \"Data/Medium\",\n",
    "    \"HARD\":   \"Data/Hard\"\n",
    "}\n",
    "\n",
    "BASE_LOCAL_DIR = os.path.join(os.getcwd(), \"data_repo\")\n",
    "\n",
    "def download_folder_from_github(user, repo, repo_folder_path, local_output_dir, branch=\"main\"):\n",
    "    \"\"\"Descarga todos los .npz de una carpeta de GitHub usando la API.\"\"\"\n",
    "    api_url = f\"https://api.github.com/repos/{user}/{repo}/contents/{repo_folder_path}?ref={branch}\"\n",
    "    \n",
    "    print(f\"üîç Consultando API para: {repo_folder_path}...\")\n",
    "    try:\n",
    "        response = requests.get(api_url)\n",
    "        if response.status_code == 404:\n",
    "            print(f\"‚ùå Error 404: No existe la carpeta '{repo_folder_path}' en la rama '{branch}'.\")\n",
    "            return local_output_dir\n",
    "        if response.status_code != 200:\n",
    "            print(f\"‚ùå Error API ({response.status_code}): {response.text}\")\n",
    "            return local_output_dir\n",
    "\n",
    "        files_list = response.json()\n",
    "        \n",
    "        if not os.path.exists(local_output_dir):\n",
    "            os.makedirs(local_output_dir)\n",
    "\n",
    "        if isinstance(files_list, dict) and 'message' in files_list:\n",
    "            print(\"‚ùå Error: La ruta parece no ser una carpeta v√°lida.\")\n",
    "            return local_output_dir\n",
    "\n",
    "        count = 0\n",
    "        for item in files_list:\n",
    "            if item['type'] == 'file' and item['name'].endswith('.npz'):\n",
    "                local_path = os.path.join(local_output_dir, item['name'])\n",
    "                if not os.path.exists(local_path):\n",
    "                    try:\n",
    "                        r = requests.get(item['download_url'])\n",
    "                        with open(local_path, 'wb') as f:\n",
    "                            f.write(r.content)\n",
    "                        count += 1\n",
    "                    except Exception as e:\n",
    "                        print(f\"  ‚ùå Fall√≥ {item['name']}: {e}\")\n",
    "                else:\n",
    "                    count += 1 # Ya exist√≠a\n",
    "        \n",
    "        print(f\"‚úÖ Fase {repo_folder_path}: {count} archivos listos en {local_output_dir}\")\n",
    "        return local_output_dir\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error de conexi√≥n: {e}\")\n",
    "        return local_output_dir\n",
    "\n",
    "# --- C. EJECUCI√ìN DE DESCARGA ---\n",
    "PATHS = {}\n",
    "print(f\"\\n‚öôÔ∏è Sincronizando con GitHub ({REPO_USER}/{REPO_NAME})...\")\n",
    "\n",
    "for phase_name, repo_path in REPO_FOLDERS.items():\n",
    "    local_target = os.path.join(BASE_LOCAL_DIR, phase_name)\n",
    "    final_path = download_folder_from_github(REPO_USER, REPO_NAME, repo_path, local_target, BRANCH)\n",
    "    PATHS[phase_name] = final_path\n",
    "\n",
    "# --- D. CURRICULUM ---\n",
    "CURRICULUM = [\n",
    "    {\"phase\": \"EASY\",   \"epochs\": 20, \"lr\": 1e-3, \"bs\": 128},\n",
    "    {\"phase\": \"MEDIUM\", \"epochs\": 15, \"lr\": 1e-4, \"bs\": 64},\n",
    "    {\"phase\": \"HARD\",   \"epochs\": 30, \"lr\": 1e-4, \"bs\": 32}\n",
    "]\n",
    "\n",
    "print(f\"\\nüìÇ Rutas configuradas correctamente.\")\n",
    "print(f\"üöÄ Listo para ejecutar el Bloque de Entrenamiento.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "ee7b3c55",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ARQUITECTURA DE INTITI (Encoder-Only)\\n===================================================================\\n\\n[ ENTRADA: Coordenadas (N ciudades) ]\\n             ‚îÇ\\n             ‚ñº\\n+-----------------------------------------+\\n|           CAPA DE EMBEDDING             |  ‚ûî Convierte (x,y) en\\n+-----------------------------------------+     vectores de alta dimensi√≥n.\\n             ‚îÇ\\n             ‚ñº\\n+-----------------------------------------+\\n|     TRANSFORMER ENCODER (x capas)       |  ‚ûî El \"Cerebro\".\\n|   [ Self-Attention + FeedForward ]      |  ‚ûî Las ciudades \"hablan\" entre s√≠\\n|                                         |     para entender el mapa global.\\n+-----------------------------------------+\\n             ‚îÇ\\n             ‚ñº\\n    [ VECTORES DE CIUDAD ENRIQUECIDOS ]      ‚ûî Ahora cada nodo tiene contexto.\\n             ‚îÇ\\n             ‚ñº\\n+-----------------------------------------+\\n|       CAPA DE PROYECCI√ìN (MLP)          |  ‚ûî La \"Calculadora de Afinidad\".\\n|    (Compara cada par de vectores)       |  ‚ûî Calcula qu√© tanto se atraen\\n+-----------------------------------------+     la Ciudad A y la Ciudad B.\\n             ‚îÇ\\n             ‚ñº\\n [ SALIDA: MATRIZ DE PROBABILIDAD NxN ]      ‚ûî Un mapa de calor de conexiones.'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"ARQUITECTURA DE INTITI (Encoder-Only)\n",
    "===================================================================\n",
    "\n",
    "[ ENTRADA: Coordenadas (N ciudades) ]\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "+-----------------------------------------+\n",
    "|           CAPA DE EMBEDDING             |  ‚ûî Convierte (x,y) en\n",
    "+-----------------------------------------+     vectores de alta dimensi√≥n.\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "+-----------------------------------------+\n",
    "|     TRANSFORMER ENCODER (x capas)       |  ‚ûî El \"Cerebro\".\n",
    "|   [ Self-Attention + FeedForward ]      |  ‚ûî Las ciudades \"hablan\" entre s√≠\n",
    "|                                         |     para entender el mapa global.\n",
    "+-----------------------------------------+\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "    [ VECTORES DE CIUDAD ENRIQUECIDOS ]      ‚ûî Ahora cada nodo tiene contexto.\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    "+-----------------------------------------+\n",
    "|       CAPA DE PROYECCI√ìN (MLP)          |  ‚ûî La \"Calculadora de Afinidad\".\n",
    "|    (Compara cada par de vectores)       |  ‚ûî Calcula qu√© tanto se atraen\n",
    "+-----------------------------------------+     la Ciudad A y la Ciudad B.\n",
    "             ‚îÇ\n",
    "             ‚ñº\n",
    " [ SALIDA: MATRIZ DE PROBABILIDAD NxN ]      ‚ûî Un mapa de calor de conexiones.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "157b0bef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ==========================================\n",
    "# 2. ARQUITECTURA DEL MODELO (ENCODER-ONLY / INTITI ARCH)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "class IntitiEncoderModel(nn.Module):\n",
    "    def __init__(self, input_dim=2, d_model=128, nhead=8, num_layers=4, dim_feedforward=512, dropout=0.1):\n",
    "        super().__init__()\n",
    "        \n",
    "        # 1. CAPA DE EMBEDDING\n",
    "        # Convierte coordenadas (x,y) en vectores de alta dimensi√≥n\n",
    "        self.embedding = nn.Linear(input_dim, d_model)\n",
    "        \n",
    "        # 2. TRANSFORMER ENCODER (El \"Cerebro\")\n",
    "        # Permite que las ciudades \"hablen\" entre s√≠ (Self-Attention)\n",
    "        encoder_layer = nn.TransformerEncoderLayer(\n",
    "            d_model=d_model,\n",
    "            nhead=nhead,\n",
    "            dim_feedforward=dim_feedforward,\n",
    "            dropout=dropout,\n",
    "            batch_first=True, # Importante: Batch en la primera dimensi√≥n\n",
    "            norm_first=True   # Estabilidad de entrenamiento (Pre-Norm)\n",
    "        )\n",
    "        self.encoder = nn.TransformerEncoder(encoder_layer, num_layers=num_layers)\n",
    "        \n",
    "        # 3. CAPA DE PROYECCI√ìN (MLP / Calculadora de Afinidad)\n",
    "        # Esta red neuronal peque√±a toma DOS ciudades y decide si se conectan.\n",
    "        # Entrada: d_model * 2 (Ciudad A + Ciudad B)\n",
    "        # Salida: 1 (Score de conexi√≥n)\n",
    "        self.edge_mlp = nn.Sequential(\n",
    "            nn.Linear(d_model * 2, d_model),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(d_model, 1) # Salida escalar (score)\n",
    "        )\n",
    "        \n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x, return_probs=False):\n",
    "        \"\"\"\n",
    "        x: [Batch, N_ciudades, 2]\n",
    "        Retorna: [Batch, N_ciudades, N_ciudades] -> Matriz de Probabilidad\n",
    "        \"\"\"\n",
    "        B, N, _ = x.size()\n",
    "        \n",
    "        # --- PASO 1: EMBEDDING ---\n",
    "        h = self.embedding(x)  # [B, N, d_model]\n",
    "        \n",
    "        # --- PASO 2: ENCODER (Contexto Global) ---\n",
    "        # Ahora 'h' contiene informaci√≥n rica de cada ciudad y sus vecinos\n",
    "        h = self.encoder(h)    # [B, N, d_model]\n",
    "        \n",
    "        # --- PASO 3: PROYECCI√ìN DE PARES (Broadcasting) ---\n",
    "        # Queremos comparar TODAS las ciudades contra TODAS las ciudades.\n",
    "        # Creamos dos vistas de los datos para combinarlos:\n",
    "        \n",
    "        # Vista Filas: Repetimos N veces hacia abajo\n",
    "        h_src = h.unsqueeze(2).expand(-1, -1, N, -1) # [B, N, N, d_model]\n",
    "        \n",
    "        # Vista Columnas: Repetimos N veces hacia el lado\n",
    "        h_tgt = h.unsqueeze(1).expand(-1, N, -1, -1) # [B, N, N, d_model]\n",
    "        \n",
    "        # Concatenamos: Ahora cada celda (i, j) tiene el vector de la ciudad i Y la ciudad j\n",
    "        h_pairs = torch.cat([h_src, h_tgt], dim=-1)  # [B, N, N, d_model * 2]\n",
    "        \n",
    "        # --- PASO 4: CALCULAR SCORES ---\n",
    "        # Pasamos cada par por la MLP\n",
    "        edge_scores = self.edge_mlp(h_pairs)         # [B, N, N, 1]\n",
    "        \n",
    "        # Quitamos la √∫ltima dimensi√≥n para que quede una matriz cuadrada\n",
    "        edge_scores = edge_scores.squeeze(-1)        # [B, N, N]\n",
    "        \n",
    "        # Mascarar la diagonal (No queremos conexiones Ciudad A -> Ciudad A)\n",
    "        # Llenamos la diagonal con un n√∫mero muy negativo\n",
    "        mask_diag = torch.eye(N, device=x.device).bool().unsqueeze(0).expand(B, -1, -1)\n",
    "        edge_scores = edge_scores.masked_fill(mask_diag, float('-1e9'))\n",
    "\n",
    "        if return_probs:\n",
    "            # Si queremos probabilidades (0 a 1), aplicamos Sigmoid\n",
    "            return torch.sigmoid(edge_scores)\n",
    "        \n",
    "        # Retornamos Logits (scores crudos) para usar con BCEWithLogitsLoss\n",
    "        return edge_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "653e886d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ==========================================\n",
    "# 3. UTILIDADES DE EVALUACI√ìN\n",
    "# ==========================================\n",
    "def calculate_gap(model, loader, device):\n",
    "    \"\"\"Calcula el Optimality GAP (%) usando Greedy Decoding en un batch.\"\"\"\n",
    "    model.eval()\n",
    "    try:\n",
    "        # Tomamos solo el primer batch para no demorar el entrenamiento\n",
    "        batch_x, batch_y = next(iter(loader))\n",
    "    except StopIteration:\n",
    "        return 0.0 # Loader vac√≠o\n",
    "\n",
    "    batch_x, batch_y = batch_x.to(device), batch_y.to(device)\n",
    "    batch_size, n_nodes, _ = batch_x.size()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Inferencia Greedy (Teacher Forcing = False)\n",
    "        # El modelo genera la secuencia de √≠ndices autom√°ticamente\n",
    "        logits = model(batch_x, teacher_forcing=False)\n",
    "        # logits: [Batch, N, N_nodes]\n",
    "\n",
    "        pred_indices = logits.argmax(dim=2) # [Batch, N]\n",
    "\n",
    "        # Stackear para formar tour\n",
    "        pred_tour = pred_indices\n",
    "\n",
    "    # --- C√°lculo de Distancias ---\n",
    "    def get_dist(pts, idx):\n",
    "        # pts: [B, N, 2], idx: [B, N]\n",
    "        gathered = torch.gather(pts, 1, idx.unsqueeze(-1).expand(-1, -1, 2))\n",
    "        next_pts = torch.roll(gathered, -1, dims=1)\n",
    "        return torch.norm(gathered - next_pts, dim=2).sum(dim=1)\n",
    "\n",
    "    cost_model = get_dist(batch_x, pred_tour)\n",
    "    cost_oracle = get_dist(batch_x, batch_y)\n",
    "\n",
    "    gap = ((cost_model - cost_oracle) / cost_oracle).mean().item() * 100\n",
    "    return gap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "e09f18aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üöÄ ENTRENAMIENTO INTITI (ENCODER-ONLY) OPTIMIZADO\n",
      "\n",
      "========================================\n",
      "üéì FASE: EASY | Epochs: 20\n",
      "========================================\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìâ Epoca 1 | Loss: 120653.40 | üìä GAP: 54.53%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìâ Epoca 2 | Loss: 120672.83 | üìä GAP: 12.08%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìâ Epoca 3 | Loss: 120583.46 | üìä GAP: 10.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìâ Epoca 4 | Loss: 120557.55 | üìä GAP: 13.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   üìâ Epoca 5 | Loss: 120648.22 | üìä GAP: 12.44%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                          \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[33]\u001b[39m\u001b[32m, line 101\u001b[39m\n\u001b[32m     98\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m i == \u001b[38;5;28mlen\u001b[39m(files) - \u001b[32m1\u001b[39m: gap_val = get_gap(model, loader, DEVICE)\n\u001b[32m    100\u001b[39m         \u001b[38;5;66;03m# Limpieza\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m101\u001b[39m         \u001b[38;5;28;01mdel\u001b[39;00m data, pts, sols, loader; gc.collect(); torch.cuda.empty_cache()\n\u001b[32m    103\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e: \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m‚ùå Error en \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mf_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    105\u001b[39m avg_loss = loss_acc / (\u001b[38;5;28mlen\u001b[39m(files) * (\u001b[32m10000\u001b[39m // stage[\u001b[33m'\u001b[39m\u001b[33mbs\u001b[39m\u001b[33m'\u001b[39m])) \u001b[38;5;66;03m# Aprox\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\torch\\cuda\\memory.py:192\u001b[39m, in \u001b[36mempty_cache\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    181\u001b[39m \u001b[38;5;250m\u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Release all unoccupied cached memory currently held by the caching\u001b[39;00m\n\u001b[32m    182\u001b[39m \u001b[33;03mallocator so that those can be used in other GPU application and visible in\u001b[39;00m\n\u001b[32m    183\u001b[39m \u001b[33;03m`nvidia-smi`.\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    189\u001b[39m \u001b[33;03m    more details about GPU memory management.\u001b[39;00m\n\u001b[32m    190\u001b[39m \u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m    191\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m is_initialized():\n\u001b[32m--> \u001b[39m\u001b[32m192\u001b[39m     \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_emptyCache\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 4. BUCLE DE ENTRENAMIENTO (INTITI ARCH)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# --- UTILIDADES COMPACTAS ---\n",
    "def tours_to_adjacency(tours, n_nodes):\n",
    "    \"\"\"Genera matriz target [B, N, N] con 1s en las aristas de la ruta.\"\"\"\n",
    "    B = tours.size(0)\n",
    "    targets = torch.zeros(B, n_nodes, n_nodes, device=tours.device)\n",
    "    # Scatter pone 1s conectando nodo i -> nodo i+1\n",
    "    targets.scatter_(2, torch.roll(tours, -1, dims=1).unsqueeze(2), 1)\n",
    "    # Ajuste fino para asegurar alineaci√≥n batch/nodo\n",
    "    for b in range(B): targets[b, tours[b], torch.roll(tours, -1, dims=1)[b]] = 1\n",
    "    return targets\n",
    "\n",
    "def decode_matrix_greedy(scores):\n",
    "    \"\"\"Greedy decoding simple: elige el vecino con mayor score no visitado.\"\"\"\n",
    "    B, N, _ = scores.size()\n",
    "    tours = torch.zeros(B, N, dtype=torch.long, device=scores.device)\n",
    "    visited = torch.zeros(B, N, dtype=torch.bool, device=scores.device)\n",
    "    visited[:, 0] = True # Empezamos en 0\n",
    "    curr = torch.zeros(B, dtype=torch.long, device=scores.device)\n",
    "    \n",
    "    for t in range(1, N):\n",
    "        # Scores de los vecinos del nodo actual\n",
    "        row = scores.gather(1, curr.view(B,1,1).expand(-1,1,N)).squeeze(1)\n",
    "        row.masked_fill_(visited, float('-inf')) # Bloquear visitados\n",
    "        next_node = row.argmax(dim=1)\n",
    "        \n",
    "        tours[:, t] = next_node\n",
    "        visited.scatter_(1, next_node.unsqueeze(1), True)\n",
    "        curr = next_node\n",
    "    return tours\n",
    "\n",
    "def get_gap(model, loader, device):\n",
    "    \"\"\"Calcula GAP en el primer batch del loader.\"\"\"\n",
    "    model.eval()\n",
    "    bx, by = next(iter(loader))\n",
    "    bx, by = bx.to(device), by.to(device)\n",
    "    with torch.no_grad():\n",
    "        tour_pred = decode_matrix_greedy(model(bx))\n",
    "    \n",
    "    # Funci√≥n lambda local para distancia\n",
    "    dist = lambda p, t: torch.norm(torch.gather(p,1,t.unsqueeze(-1).expand(-1,-1,2)) - torch.roll(torch.gather(p,1,t.unsqueeze(-1).expand(-1,-1,2)),-1,dims=1), dim=2).sum(1)\n",
    "    return ((dist(bx, tour_pred) - dist(bx, by)) / dist(bx, by)).mean().item() * 100\n",
    "\n",
    "# ---------------------------------------------------------\n",
    "# ENTRENAMIENTO\n",
    "# ---------------------------------------------------------\n",
    "model = IntitiEncoderModel(input_dim=2, d_model=128, nhead=8, num_layers=4, dim_feedforward=512).to(DEVICE)\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = None # Se define en el bucle por fase\n",
    "\n",
    "print(\"\\nüöÄ ENTRENAMIENTO INTITI (ENCODER-ONLY) OPTIMIZADO\")\n",
    "\n",
    "for stage in CURRICULUM:\n",
    "    phase, folder = stage['phase'], PATHS[stage['phase']]\n",
    "    files = glob.glob(os.path.join(folder, \"*.npz\"))\n",
    "    if not files: print(f\"‚ö†Ô∏è Salto {phase}: Sin datos.\"); continue\n",
    "    \n",
    "    print(f\"\\n{'='*40}\\nüéì FASE: {phase} | Epochs: {stage['epochs']}\\n{'='*40}\")\n",
    "    \n",
    "    optimizer = optim.Adam(model.parameters(), lr=stage['lr'])\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', patience=3, factor=0.5)\n",
    "\n",
    "    for epoch in range(stage['epochs']):\n",
    "        model.train()\n",
    "        loss_acc = 0\n",
    "        gap_val = 0\n",
    "        \n",
    "        # Iteramos archivos (Lazy Loading)\n",
    "        for i, f_path in enumerate(files):\n",
    "            try:\n",
    "                # Carga y Normalizaci√≥n r√°pida\n",
    "                data = np.load(f_path)\n",
    "                pts = torch.from_numpy(data['points']).float()\n",
    "                sols = torch.from_numpy(data['solutions']).long()\n",
    "                if pts.max() > 1.0: pts /= pts.max()\n",
    "                \n",
    "                loader = DataLoader(TensorDataset(pts, sols), batch_size=stage['bs'], shuffle=True)\n",
    "                \n",
    "                # Bucle de Batches\n",
    "                for bx, by in tqdm(loader, desc=f\"Ep {epoch+1} | {os.path.basename(f_path)}\", leave=False):\n",
    "                    bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "                    \n",
    "                    optimizer.zero_grad()\n",
    "                    preds = model(bx) # [B, N, N]\n",
    "                    target = tours_to_adjacency(by, bx.size(1)) # [B, N, N]\n",
    "                    \n",
    "                    loss = criterion(preds, target)\n",
    "                    loss.backward()\n",
    "                    torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
    "                    optimizer.step()\n",
    "                    loss_acc += loss.item()\n",
    "\n",
    "                # Calcular GAP solo en el √∫ltimo archivo para validar\n",
    "                if i == len(files) - 1: gap_val = get_gap(model, loader, DEVICE)\n",
    "\n",
    "                # Limpieza\n",
    "                del data, pts, sols, loader; gc.collect(); torch.cuda.empty_cache()\n",
    "\n",
    "            except Exception as e: print(f\"‚ùå Error en {f_path}: {e}\")\n",
    "\n",
    "        avg_loss = loss_acc / (len(files) * (10000 // stage['bs'])) # Aprox\n",
    "        print(f\"   üìâ Epoca {epoch+1} | Loss: {avg_loss:.2f} | üìä GAP: {gap_val:.2f}%\")\n",
    "        \n",
    "        scheduler.step(avg_loss)\n",
    "        torch.save(model.state_dict(), os.path.join(folder, f\"checkpoint_{phase}_best.pth\"))\n",
    "\n",
    "print(\"\\nüèÜ FIN.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "e60dd6b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      "üìä VALIDANDO FASE: EASY (MODO COMPLETO)\n",
      "============================================================\n",
      "‚ùå Error cargando modelo: Error(s) in loading state_dict for EncoderPointerModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.input_proj.weight\", \"encoder.input_proj.bias\", \"encoder.encoder.layers.0.self_attn.in_proj_weight\", \"encoder.encoder.layers.0.self_attn.in_proj_bias\", \"encoder.encoder.layers.0.self_attn.out_proj.weight\", \"encoder.encoder.layers.0.self_attn.out_proj.bias\", \"encoder.encoder.layers.0.linear1.weight\", \"encoder.encoder.layers.0.linear1.bias\", \"encoder.encoder.layers.0.linear2.weight\", \"encoder.encoder.layers.0.linear2.bias\", \"encoder.encoder.layers.0.norm1.weight\", \"encoder.encoder.layers.0.norm1.bias\", \"encoder.encoder.layers.0.norm2.weight\", \"encoder.encoder.layers.0.norm2.bias\", \"encoder.encoder.layers.1.self_attn.in_proj_weight\", \"encoder.encoder.layers.1.self_attn.in_proj_bias\", \"encoder.encoder.layers.1.self_attn.out_proj.weight\", \"encoder.encoder.layers.1.self_attn.out_proj.bias\", \"encoder.encoder.layers.1.linear1.weight\", \"encoder.encoder.layers.1.linear1.bias\", \"encoder.encoder.layers.1.linear2.weight\", \"encoder.encoder.layers.1.linear2.bias\", \"encoder.encoder.layers.1.norm1.weight\", \"encoder.encoder.layers.1.norm1.bias\", \"encoder.encoder.layers.1.norm2.weight\", \"encoder.encoder.layers.1.norm2.bias\", \"encoder.encoder.layers.2.self_attn.in_proj_weight\", \"encoder.encoder.layers.2.self_attn.in_proj_bias\", \"encoder.encoder.layers.2.self_attn.out_proj.weight\", \"encoder.encoder.layers.2.self_attn.out_proj.bias\", \"encoder.encoder.layers.2.linear1.weight\", \"encoder.encoder.layers.2.linear1.bias\", \"encoder.encoder.layers.2.linear2.weight\", \"encoder.encoder.layers.2.linear2.bias\", \"encoder.encoder.layers.2.norm1.weight\", \"encoder.encoder.layers.2.norm1.bias\", \"encoder.encoder.layers.2.norm2.weight\", \"encoder.encoder.layers.2.norm2.bias\", \"decoder.start_token\", \"decoder.step_emb.weight\", \"decoder.decoder.layers.0.self_attn.in_proj_weight\", \"decoder.decoder.layers.0.self_attn.in_proj_bias\", \"decoder.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.decoder.layers.0.self_attn.out_proj.bias\", \"decoder.decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.0.linear1.weight\", \"decoder.decoder.layers.0.linear1.bias\", \"decoder.decoder.layers.0.linear2.weight\", \"decoder.decoder.layers.0.linear2.bias\", \"decoder.decoder.layers.0.norm1.weight\", \"decoder.decoder.layers.0.norm1.bias\", \"decoder.decoder.layers.0.norm2.weight\", \"decoder.decoder.layers.0.norm2.bias\", \"decoder.decoder.layers.0.norm3.weight\", \"decoder.decoder.layers.0.norm3.bias\", \"decoder.decoder.layers.1.self_attn.in_proj_weight\", \"decoder.decoder.layers.1.self_attn.in_proj_bias\", \"decoder.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.decoder.layers.1.self_attn.out_proj.bias\", \"decoder.decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.1.linear1.weight\", \"decoder.decoder.layers.1.linear1.bias\", \"decoder.decoder.layers.1.linear2.weight\", \"decoder.decoder.layers.1.linear2.bias\", \"decoder.decoder.layers.1.norm1.weight\", \"decoder.decoder.layers.1.norm1.bias\", \"decoder.decoder.layers.1.norm2.weight\", \"decoder.decoder.layers.1.norm2.bias\", \"decoder.decoder.layers.1.norm3.weight\", \"decoder.decoder.layers.1.norm3.bias\", \"decoder.query_proj.weight\", \"decoder.query_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embedding.weight\", \"embedding.bias\", \"edge_mlp.0.weight\", \"edge_mlp.0.bias\", \"edge_mlp.2.weight\", \"edge_mlp.2.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\". \n",
      "\n",
      "============================================================\n",
      "üìä VALIDANDO FASE: MEDIUM (MODO COMPLETO)\n",
      "============================================================\n",
      "‚ùå Error cargando modelo: Error(s) in loading state_dict for EncoderPointerModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.input_proj.weight\", \"encoder.input_proj.bias\", \"encoder.encoder.layers.0.self_attn.in_proj_weight\", \"encoder.encoder.layers.0.self_attn.in_proj_bias\", \"encoder.encoder.layers.0.self_attn.out_proj.weight\", \"encoder.encoder.layers.0.self_attn.out_proj.bias\", \"encoder.encoder.layers.0.linear1.weight\", \"encoder.encoder.layers.0.linear1.bias\", \"encoder.encoder.layers.0.linear2.weight\", \"encoder.encoder.layers.0.linear2.bias\", \"encoder.encoder.layers.0.norm1.weight\", \"encoder.encoder.layers.0.norm1.bias\", \"encoder.encoder.layers.0.norm2.weight\", \"encoder.encoder.layers.0.norm2.bias\", \"encoder.encoder.layers.1.self_attn.in_proj_weight\", \"encoder.encoder.layers.1.self_attn.in_proj_bias\", \"encoder.encoder.layers.1.self_attn.out_proj.weight\", \"encoder.encoder.layers.1.self_attn.out_proj.bias\", \"encoder.encoder.layers.1.linear1.weight\", \"encoder.encoder.layers.1.linear1.bias\", \"encoder.encoder.layers.1.linear2.weight\", \"encoder.encoder.layers.1.linear2.bias\", \"encoder.encoder.layers.1.norm1.weight\", \"encoder.encoder.layers.1.norm1.bias\", \"encoder.encoder.layers.1.norm2.weight\", \"encoder.encoder.layers.1.norm2.bias\", \"encoder.encoder.layers.2.self_attn.in_proj_weight\", \"encoder.encoder.layers.2.self_attn.in_proj_bias\", \"encoder.encoder.layers.2.self_attn.out_proj.weight\", \"encoder.encoder.layers.2.self_attn.out_proj.bias\", \"encoder.encoder.layers.2.linear1.weight\", \"encoder.encoder.layers.2.linear1.bias\", \"encoder.encoder.layers.2.linear2.weight\", \"encoder.encoder.layers.2.linear2.bias\", \"encoder.encoder.layers.2.norm1.weight\", \"encoder.encoder.layers.2.norm1.bias\", \"encoder.encoder.layers.2.norm2.weight\", \"encoder.encoder.layers.2.norm2.bias\", \"decoder.start_token\", \"decoder.step_emb.weight\", \"decoder.decoder.layers.0.self_attn.in_proj_weight\", \"decoder.decoder.layers.0.self_attn.in_proj_bias\", \"decoder.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.decoder.layers.0.self_attn.out_proj.bias\", \"decoder.decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.0.linear1.weight\", \"decoder.decoder.layers.0.linear1.bias\", \"decoder.decoder.layers.0.linear2.weight\", \"decoder.decoder.layers.0.linear2.bias\", \"decoder.decoder.layers.0.norm1.weight\", \"decoder.decoder.layers.0.norm1.bias\", \"decoder.decoder.layers.0.norm2.weight\", \"decoder.decoder.layers.0.norm2.bias\", \"decoder.decoder.layers.0.norm3.weight\", \"decoder.decoder.layers.0.norm3.bias\", \"decoder.decoder.layers.1.self_attn.in_proj_weight\", \"decoder.decoder.layers.1.self_attn.in_proj_bias\", \"decoder.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.decoder.layers.1.self_attn.out_proj.bias\", \"decoder.decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.1.linear1.weight\", \"decoder.decoder.layers.1.linear1.bias\", \"decoder.decoder.layers.1.linear2.weight\", \"decoder.decoder.layers.1.linear2.bias\", \"decoder.decoder.layers.1.norm1.weight\", \"decoder.decoder.layers.1.norm1.bias\", \"decoder.decoder.layers.1.norm2.weight\", \"decoder.decoder.layers.1.norm2.bias\", \"decoder.decoder.layers.1.norm3.weight\", \"decoder.decoder.layers.1.norm3.bias\", \"decoder.query_proj.weight\", \"decoder.query_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embedding.weight\", \"embedding.bias\", \"edge_mlp.0.weight\", \"edge_mlp.0.bias\", \"edge_mlp.2.weight\", \"edge_mlp.2.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\". \n",
      "\n",
      "============================================================\n",
      "üìä VALIDANDO FASE: HARD (MODO COMPLETO)\n",
      "============================================================\n",
      "‚ö†Ô∏è Salto Fase: No existe checkpoint en data_repo/HARD/checkpoint_HARD_best.pth\n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 5. VALIDACI√ìN FINAL COMPLETA (MULTI-PART)\n",
    "# ==========================================\n",
    "\n",
    "# --- CONFIGURACI√ìN DE RUTAS ---\n",
    "PATHS_CONFIG = {\n",
    "    \"EASY\": {\n",
    "        \"ckpt\": \"data_repo/EASY/checkpoint_EASY_best.pth\",\n",
    "        \"val_folder\": \"Data/Validation/Easy\",\n",
    "        \"val_prefix\": \"tsp_easy\"\n",
    "    },\n",
    "    \"MEDIUM\": {\n",
    "        \"ckpt\": \"data_repo/MEDIUM/checkpoint_MEDIUM_best.pth\",\n",
    "        \"val_folder\": \"Data/Validation/Medium\",\n",
    "        \"val_prefix\": \"tsp_medium\"\n",
    "    },\n",
    "    \"HARD\": {\n",
    "        \"ckpt\": \"data_repo/HARD/checkpoint_HARD_best.pth\",\n",
    "        \"val_folder\": \"Data/Validation/Hard\",\n",
    "        \"val_prefix\": \"tsp_hard\"\n",
    "    }\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def get_tour_distance(points, tour_indices):\n",
    "    \"\"\"Calcula distancia total de la ruta.\"\"\"\n",
    "    gathered = torch.gather(points, 1, tour_indices.unsqueeze(-1).expand(-1, -1, 2))\n",
    "    next_pts = torch.roll(gathered, -1, dims=1)\n",
    "    dist = torch.norm(gathered - next_pts, dim=2).sum(dim=1)\n",
    "    return dist\n",
    "\n",
    "def load_all_validation_parts(folder, prefix):\n",
    "    \"\"\"\n",
    "    Busca TODAS las partes (part_0, part_1...) y las une en un solo dataset gigante.\n",
    "    \"\"\"\n",
    "    if not os.path.exists(folder):\n",
    "        print(f\"‚ùå Carpeta no existe: {folder}\")\n",
    "        return None, None\n",
    "    \n",
    "    # Buscar todos los archivos que coincidan\n",
    "    search_pattern = os.path.join(folder, f\"{prefix}*.npz\")\n",
    "    all_files = sorted(glob.glob(search_pattern))\n",
    "    \n",
    "    if not all_files:\n",
    "        print(f\"‚ùå No encontr√© archivos {prefix}*.npz en {folder}\")\n",
    "        return None, None\n",
    "    \n",
    "    print(f\"üìö Uniendo {len(all_files)} archivos de validaci√≥n encontrados...\")\n",
    "    \n",
    "    all_points = []\n",
    "    all_solutions = []\n",
    "    \n",
    "    for f_path in all_files:\n",
    "        try:\n",
    "            data = np.load(f_path, allow_pickle=True)\n",
    "            all_points.append(data['points'])\n",
    "            \n",
    "            # Conversi√≥n m√°gica de lista de objetos a matriz int64\n",
    "            raw_sols = data['solutions']\n",
    "            # Verificamos si ya es matriz o lista de listas\n",
    "            if raw_sols.dtype == np.object_:\n",
    "                sols_mat = np.vstack(raw_sols).astype(np.int64)\n",
    "            else:\n",
    "                sols_mat = raw_sols.astype(np.int64)\n",
    "                \n",
    "            all_solutions.append(sols_mat)\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error leyendo {os.path.basename(f_path)}: {e}\")\n",
    "            continue\n",
    "\n",
    "    if not all_points:\n",
    "        return None, None\n",
    "\n",
    "    # Pegamos todo en arrays gigantes\n",
    "    # np.concatenate une los arrays uno detr√°s de otro\n",
    "    final_points = np.concatenate(all_points) \n",
    "    final_solutions = np.concatenate(all_solutions)\n",
    "    \n",
    "    return torch.FloatTensor(final_points), torch.from_numpy(final_solutions)\n",
    "\n",
    "def validate_phase(phase_name, config):\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"üìä VALIDANDO FASE: {phase_name} (MODO COMPLETO)\")\n",
    "    print(f\"{'='*60}\")\n",
    "\n",
    "    # 1. Cargar Checkpoint\n",
    "    if not os.path.exists(config[\"ckpt\"]):\n",
    "        print(f\"‚ö†Ô∏è Salto Fase: No existe checkpoint en {config['ckpt']}\")\n",
    "        return\n",
    "\n",
    "    model = EncoderPointerModel(input_dim=2, d_model=128, nhead=8, enc_layers=3, dec_layers=2, max_seq_len=150).to(DEVICE)\n",
    "    \n",
    "    try:\n",
    "        model.load_state_dict(torch.load(config[\"ckpt\"], map_location=DEVICE, weights_only=False))\n",
    "        model.eval()\n",
    "        print(f\"üß† Modelo cargado: {os.path.basename(config['ckpt'])}\")\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando modelo: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Cargar TODA la data\n",
    "    points, solutions = load_all_validation_parts(config[\"val_folder\"], config[\"val_prefix\"])\n",
    "    \n",
    "    if points is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"üìÇ Total muestras cargadas: {len(points)}\")\n",
    "    \n",
    "    # Normalizaci√≥n\n",
    "    if points.max() > 1.0: points /= points.max()\n",
    "\n",
    "    # Dataset completo\n",
    "    dataset = TensorDataset(points, solutions)\n",
    "    loader = DataLoader(dataset, batch_size=64, shuffle=False) # Batch grande para ir r√°pido\n",
    "\n",
    "    # 3. Inferencia\n",
    "    gap_accum = 0\n",
    "    total_samples = 0\n",
    "    \n",
    "    pbar = tqdm(loader, desc=\"Benchmarking\")\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for bx, by in pbar:\n",
    "            bx, by = bx.to(DEVICE), by.to(DEVICE)\n",
    "            \n",
    "            logits = model(bx, teacher_forcing=False)\n",
    "            pred_tour = logits.argmax(dim=2) \n",
    "\n",
    "            cost_model = get_tour_distance(bx, pred_tour)\n",
    "            cost_ortools = get_tour_distance(bx, by)\n",
    "\n",
    "            gap = ((cost_model - cost_ortools) / cost_ortools)\n",
    "            gap_accum += gap.sum().item()\n",
    "            total_samples += bx.size(0)\n",
    "            \n",
    "            pbar.set_postfix({'GAP Acum': f\"{(gap_accum/total_samples)*100:.2f}%\"})\n",
    "\n",
    "    final_gap = (gap_accum / total_samples) * 100\n",
    "    print(f\"\\nüèÜ RESULTADO FINAL {phase_name}: GAP GLOBAL {final_gap:.2f}%\")\n",
    "\n",
    "# --- EJECUTAR ---\n",
    "# Nota: Como detuviste el entrenamiento en MEDIUM, probablemente solo EASY funcione bien.\n",
    "for phase in [\"EASY\", \"MEDIUM\", \"HARD\"]:\n",
    "    validate_phase(phase, PATHS_CONFIG[phase])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cd4b9b33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "############################################################\n",
      "üß™ EXPERIMENTO: ¬øPuede un modelo de 20 ciudades resolver uno de 50?\n",
      "############################################################\n",
      "\n",
      "============================================================\n",
      "üìä VALIDANDO FASE: GENERALIZATION_TEST (MODO COMPLETO)\n",
      "============================================================\n",
      "‚ùå Error cargando modelo: Error(s) in loading state_dict for EncoderPointerModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.input_proj.weight\", \"encoder.input_proj.bias\", \"encoder.encoder.layers.0.self_attn.in_proj_weight\", \"encoder.encoder.layers.0.self_attn.in_proj_bias\", \"encoder.encoder.layers.0.self_attn.out_proj.weight\", \"encoder.encoder.layers.0.self_attn.out_proj.bias\", \"encoder.encoder.layers.0.linear1.weight\", \"encoder.encoder.layers.0.linear1.bias\", \"encoder.encoder.layers.0.linear2.weight\", \"encoder.encoder.layers.0.linear2.bias\", \"encoder.encoder.layers.0.norm1.weight\", \"encoder.encoder.layers.0.norm1.bias\", \"encoder.encoder.layers.0.norm2.weight\", \"encoder.encoder.layers.0.norm2.bias\", \"encoder.encoder.layers.1.self_attn.in_proj_weight\", \"encoder.encoder.layers.1.self_attn.in_proj_bias\", \"encoder.encoder.layers.1.self_attn.out_proj.weight\", \"encoder.encoder.layers.1.self_attn.out_proj.bias\", \"encoder.encoder.layers.1.linear1.weight\", \"encoder.encoder.layers.1.linear1.bias\", \"encoder.encoder.layers.1.linear2.weight\", \"encoder.encoder.layers.1.linear2.bias\", \"encoder.encoder.layers.1.norm1.weight\", \"encoder.encoder.layers.1.norm1.bias\", \"encoder.encoder.layers.1.norm2.weight\", \"encoder.encoder.layers.1.norm2.bias\", \"encoder.encoder.layers.2.self_attn.in_proj_weight\", \"encoder.encoder.layers.2.self_attn.in_proj_bias\", \"encoder.encoder.layers.2.self_attn.out_proj.weight\", \"encoder.encoder.layers.2.self_attn.out_proj.bias\", \"encoder.encoder.layers.2.linear1.weight\", \"encoder.encoder.layers.2.linear1.bias\", \"encoder.encoder.layers.2.linear2.weight\", \"encoder.encoder.layers.2.linear2.bias\", \"encoder.encoder.layers.2.norm1.weight\", \"encoder.encoder.layers.2.norm1.bias\", \"encoder.encoder.layers.2.norm2.weight\", \"encoder.encoder.layers.2.norm2.bias\", \"decoder.start_token\", \"decoder.step_emb.weight\", \"decoder.decoder.layers.0.self_attn.in_proj_weight\", \"decoder.decoder.layers.0.self_attn.in_proj_bias\", \"decoder.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.decoder.layers.0.self_attn.out_proj.bias\", \"decoder.decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.0.linear1.weight\", \"decoder.decoder.layers.0.linear1.bias\", \"decoder.decoder.layers.0.linear2.weight\", \"decoder.decoder.layers.0.linear2.bias\", \"decoder.decoder.layers.0.norm1.weight\", \"decoder.decoder.layers.0.norm1.bias\", \"decoder.decoder.layers.0.norm2.weight\", \"decoder.decoder.layers.0.norm2.bias\", \"decoder.decoder.layers.0.norm3.weight\", \"decoder.decoder.layers.0.norm3.bias\", \"decoder.decoder.layers.1.self_attn.in_proj_weight\", \"decoder.decoder.layers.1.self_attn.in_proj_bias\", \"decoder.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.decoder.layers.1.self_attn.out_proj.bias\", \"decoder.decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.1.linear1.weight\", \"decoder.decoder.layers.1.linear1.bias\", \"decoder.decoder.layers.1.linear2.weight\", \"decoder.decoder.layers.1.linear2.bias\", \"decoder.decoder.layers.1.norm1.weight\", \"decoder.decoder.layers.1.norm1.bias\", \"decoder.decoder.layers.1.norm2.weight\", \"decoder.decoder.layers.1.norm2.bias\", \"decoder.decoder.layers.1.norm3.weight\", \"decoder.decoder.layers.1.norm3.bias\", \"decoder.query_proj.weight\", \"decoder.query_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embedding.weight\", \"embedding.bias\", \"edge_mlp.0.weight\", \"edge_mlp.0.bias\", \"edge_mlp.2.weight\", \"edge_mlp.2.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\". \n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# üß™ PRUEBA DE GENERALIZACI√ìN (EASY -> MEDIUM)\n",
    "# ==========================================\n",
    "\n",
    "# Definimos una configuraci√≥n h√≠brida:\n",
    "# üß† CEREBRO: Checkpoint de EASY (Entrenado con 20 nodos)\n",
    "# üìù EXAMEN: Datos de MEDIUM (Problemas de 50 nodos)\n",
    "\n",
    "CROSS_TEST_CONFIG = {\n",
    "    \"ckpt\": \"data_repo/EASY/checkpoint_EASY_best.pth\",   # Usamos el modelo peque√±o\n",
    "    \"val_folder\": \"Data/Validation/Medium\",              # Usamos la data mediana\n",
    "    \"val_prefix\": \"tsp_medium\"\n",
    "}\n",
    "\n",
    "print(f\"\\n{'#'*60}\")\n",
    "print(\"üß™ EXPERIMENTO: ¬øPuede un modelo de 20 ciudades resolver uno de 50?\")\n",
    "print(f\"{'#'*60}\")\n",
    "\n",
    "# Llamamos a tu funci√≥n de validaci√≥n existente\n",
    "validate_phase(\"GENERALIZATION_TEST\", CROSS_TEST_CONFIG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5bc9e1ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "üé® GENERANDO VISUALIZACIONES PARA: EASY\n",
      "‚ùå Error cargando modelo: Error(s) in loading state_dict for EncoderPointerModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.input_proj.weight\", \"encoder.input_proj.bias\", \"encoder.encoder.layers.0.self_attn.in_proj_weight\", \"encoder.encoder.layers.0.self_attn.in_proj_bias\", \"encoder.encoder.layers.0.self_attn.out_proj.weight\", \"encoder.encoder.layers.0.self_attn.out_proj.bias\", \"encoder.encoder.layers.0.linear1.weight\", \"encoder.encoder.layers.0.linear1.bias\", \"encoder.encoder.layers.0.linear2.weight\", \"encoder.encoder.layers.0.linear2.bias\", \"encoder.encoder.layers.0.norm1.weight\", \"encoder.encoder.layers.0.norm1.bias\", \"encoder.encoder.layers.0.norm2.weight\", \"encoder.encoder.layers.0.norm2.bias\", \"encoder.encoder.layers.1.self_attn.in_proj_weight\", \"encoder.encoder.layers.1.self_attn.in_proj_bias\", \"encoder.encoder.layers.1.self_attn.out_proj.weight\", \"encoder.encoder.layers.1.self_attn.out_proj.bias\", \"encoder.encoder.layers.1.linear1.weight\", \"encoder.encoder.layers.1.linear1.bias\", \"encoder.encoder.layers.1.linear2.weight\", \"encoder.encoder.layers.1.linear2.bias\", \"encoder.encoder.layers.1.norm1.weight\", \"encoder.encoder.layers.1.norm1.bias\", \"encoder.encoder.layers.1.norm2.weight\", \"encoder.encoder.layers.1.norm2.bias\", \"encoder.encoder.layers.2.self_attn.in_proj_weight\", \"encoder.encoder.layers.2.self_attn.in_proj_bias\", \"encoder.encoder.layers.2.self_attn.out_proj.weight\", \"encoder.encoder.layers.2.self_attn.out_proj.bias\", \"encoder.encoder.layers.2.linear1.weight\", \"encoder.encoder.layers.2.linear1.bias\", \"encoder.encoder.layers.2.linear2.weight\", \"encoder.encoder.layers.2.linear2.bias\", \"encoder.encoder.layers.2.norm1.weight\", \"encoder.encoder.layers.2.norm1.bias\", \"encoder.encoder.layers.2.norm2.weight\", \"encoder.encoder.layers.2.norm2.bias\", \"decoder.start_token\", \"decoder.step_emb.weight\", \"decoder.decoder.layers.0.self_attn.in_proj_weight\", \"decoder.decoder.layers.0.self_attn.in_proj_bias\", \"decoder.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.decoder.layers.0.self_attn.out_proj.bias\", \"decoder.decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.0.linear1.weight\", \"decoder.decoder.layers.0.linear1.bias\", \"decoder.decoder.layers.0.linear2.weight\", \"decoder.decoder.layers.0.linear2.bias\", \"decoder.decoder.layers.0.norm1.weight\", \"decoder.decoder.layers.0.norm1.bias\", \"decoder.decoder.layers.0.norm2.weight\", \"decoder.decoder.layers.0.norm2.bias\", \"decoder.decoder.layers.0.norm3.weight\", \"decoder.decoder.layers.0.norm3.bias\", \"decoder.decoder.layers.1.self_attn.in_proj_weight\", \"decoder.decoder.layers.1.self_attn.in_proj_bias\", \"decoder.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.decoder.layers.1.self_attn.out_proj.bias\", \"decoder.decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.1.linear1.weight\", \"decoder.decoder.layers.1.linear1.bias\", \"decoder.decoder.layers.1.linear2.weight\", \"decoder.decoder.layers.1.linear2.bias\", \"decoder.decoder.layers.1.norm1.weight\", \"decoder.decoder.layers.1.norm1.bias\", \"decoder.decoder.layers.1.norm2.weight\", \"decoder.decoder.layers.1.norm2.bias\", \"decoder.decoder.layers.1.norm3.weight\", \"decoder.decoder.layers.1.norm3.bias\", \"decoder.query_proj.weight\", \"decoder.query_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embedding.weight\", \"embedding.bias\", \"edge_mlp.0.weight\", \"edge_mlp.0.bias\", \"edge_mlp.2.weight\", \"edge_mlp.2.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\". \n",
      "\n",
      "üé® GENERANDO VISUALIZACIONES PARA: MEDIUM\n",
      "‚ùå Error cargando modelo: Error(s) in loading state_dict for EncoderPointerModel:\n",
      "\tMissing key(s) in state_dict: \"encoder.input_proj.weight\", \"encoder.input_proj.bias\", \"encoder.encoder.layers.0.self_attn.in_proj_weight\", \"encoder.encoder.layers.0.self_attn.in_proj_bias\", \"encoder.encoder.layers.0.self_attn.out_proj.weight\", \"encoder.encoder.layers.0.self_attn.out_proj.bias\", \"encoder.encoder.layers.0.linear1.weight\", \"encoder.encoder.layers.0.linear1.bias\", \"encoder.encoder.layers.0.linear2.weight\", \"encoder.encoder.layers.0.linear2.bias\", \"encoder.encoder.layers.0.norm1.weight\", \"encoder.encoder.layers.0.norm1.bias\", \"encoder.encoder.layers.0.norm2.weight\", \"encoder.encoder.layers.0.norm2.bias\", \"encoder.encoder.layers.1.self_attn.in_proj_weight\", \"encoder.encoder.layers.1.self_attn.in_proj_bias\", \"encoder.encoder.layers.1.self_attn.out_proj.weight\", \"encoder.encoder.layers.1.self_attn.out_proj.bias\", \"encoder.encoder.layers.1.linear1.weight\", \"encoder.encoder.layers.1.linear1.bias\", \"encoder.encoder.layers.1.linear2.weight\", \"encoder.encoder.layers.1.linear2.bias\", \"encoder.encoder.layers.1.norm1.weight\", \"encoder.encoder.layers.1.norm1.bias\", \"encoder.encoder.layers.1.norm2.weight\", \"encoder.encoder.layers.1.norm2.bias\", \"encoder.encoder.layers.2.self_attn.in_proj_weight\", \"encoder.encoder.layers.2.self_attn.in_proj_bias\", \"encoder.encoder.layers.2.self_attn.out_proj.weight\", \"encoder.encoder.layers.2.self_attn.out_proj.bias\", \"encoder.encoder.layers.2.linear1.weight\", \"encoder.encoder.layers.2.linear1.bias\", \"encoder.encoder.layers.2.linear2.weight\", \"encoder.encoder.layers.2.linear2.bias\", \"encoder.encoder.layers.2.norm1.weight\", \"encoder.encoder.layers.2.norm1.bias\", \"encoder.encoder.layers.2.norm2.weight\", \"encoder.encoder.layers.2.norm2.bias\", \"decoder.start_token\", \"decoder.step_emb.weight\", \"decoder.decoder.layers.0.self_attn.in_proj_weight\", \"decoder.decoder.layers.0.self_attn.in_proj_bias\", \"decoder.decoder.layers.0.self_attn.out_proj.weight\", \"decoder.decoder.layers.0.self_attn.out_proj.bias\", \"decoder.decoder.layers.0.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.0.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.0.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.0.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.0.linear1.weight\", \"decoder.decoder.layers.0.linear1.bias\", \"decoder.decoder.layers.0.linear2.weight\", \"decoder.decoder.layers.0.linear2.bias\", \"decoder.decoder.layers.0.norm1.weight\", \"decoder.decoder.layers.0.norm1.bias\", \"decoder.decoder.layers.0.norm2.weight\", \"decoder.decoder.layers.0.norm2.bias\", \"decoder.decoder.layers.0.norm3.weight\", \"decoder.decoder.layers.0.norm3.bias\", \"decoder.decoder.layers.1.self_attn.in_proj_weight\", \"decoder.decoder.layers.1.self_attn.in_proj_bias\", \"decoder.decoder.layers.1.self_attn.out_proj.weight\", \"decoder.decoder.layers.1.self_attn.out_proj.bias\", \"decoder.decoder.layers.1.multihead_attn.in_proj_weight\", \"decoder.decoder.layers.1.multihead_attn.in_proj_bias\", \"decoder.decoder.layers.1.multihead_attn.out_proj.weight\", \"decoder.decoder.layers.1.multihead_attn.out_proj.bias\", \"decoder.decoder.layers.1.linear1.weight\", \"decoder.decoder.layers.1.linear1.bias\", \"decoder.decoder.layers.1.linear2.weight\", \"decoder.decoder.layers.1.linear2.bias\", \"decoder.decoder.layers.1.norm1.weight\", \"decoder.decoder.layers.1.norm1.bias\", \"decoder.decoder.layers.1.norm2.weight\", \"decoder.decoder.layers.1.norm2.bias\", \"decoder.decoder.layers.1.norm3.weight\", \"decoder.decoder.layers.1.norm3.bias\", \"decoder.query_proj.weight\", \"decoder.query_proj.bias\". \n",
      "\tUnexpected key(s) in state_dict: \"embedding.weight\", \"embedding.bias\", \"edge_mlp.0.weight\", \"edge_mlp.0.bias\", \"edge_mlp.2.weight\", \"edge_mlp.2.bias\", \"encoder.layers.0.self_attn.in_proj_weight\", \"encoder.layers.0.self_attn.in_proj_bias\", \"encoder.layers.0.self_attn.out_proj.weight\", \"encoder.layers.0.self_attn.out_proj.bias\", \"encoder.layers.0.linear1.weight\", \"encoder.layers.0.linear1.bias\", \"encoder.layers.0.linear2.weight\", \"encoder.layers.0.linear2.bias\", \"encoder.layers.0.norm1.weight\", \"encoder.layers.0.norm1.bias\", \"encoder.layers.0.norm2.weight\", \"encoder.layers.0.norm2.bias\", \"encoder.layers.1.self_attn.in_proj_weight\", \"encoder.layers.1.self_attn.in_proj_bias\", \"encoder.layers.1.self_attn.out_proj.weight\", \"encoder.layers.1.self_attn.out_proj.bias\", \"encoder.layers.1.linear1.weight\", \"encoder.layers.1.linear1.bias\", \"encoder.layers.1.linear2.weight\", \"encoder.layers.1.linear2.bias\", \"encoder.layers.1.norm1.weight\", \"encoder.layers.1.norm1.bias\", \"encoder.layers.1.norm2.weight\", \"encoder.layers.1.norm2.bias\", \"encoder.layers.2.self_attn.in_proj_weight\", \"encoder.layers.2.self_attn.in_proj_bias\", \"encoder.layers.2.self_attn.out_proj.weight\", \"encoder.layers.2.self_attn.out_proj.bias\", \"encoder.layers.2.linear1.weight\", \"encoder.layers.2.linear1.bias\", \"encoder.layers.2.linear2.weight\", \"encoder.layers.2.linear2.bias\", \"encoder.layers.2.norm1.weight\", \"encoder.layers.2.norm1.bias\", \"encoder.layers.2.norm2.weight\", \"encoder.layers.2.norm2.bias\", \"encoder.layers.3.self_attn.in_proj_weight\", \"encoder.layers.3.self_attn.in_proj_bias\", \"encoder.layers.3.self_attn.out_proj.weight\", \"encoder.layers.3.self_attn.out_proj.bias\", \"encoder.layers.3.linear1.weight\", \"encoder.layers.3.linear1.bias\", \"encoder.layers.3.linear2.weight\", \"encoder.layers.3.linear2.bias\", \"encoder.layers.3.norm1.weight\", \"encoder.layers.3.norm1.bias\", \"encoder.layers.3.norm2.weight\", \"encoder.layers.3.norm2.bias\". \n"
     ]
    }
   ],
   "source": [
    "# ==========================================\n",
    "# 6. VISUALIZACI√ìN COMPARATIVA (VISUALIZER)\n",
    "# ==========================================\n",
    "\n",
    "\n",
    "# --- CONFIGURACI√ìN ---\n",
    "# Usamos la misma configuraci√≥n de rutas que antes\n",
    "PATHS_CONFIG = {\n",
    "    \"EASY\":   {\"ckpt\": \"data_repo/EASY/checkpoint_EASY_best.pth\",   \"val_folder\": \"Data/Validation/Easy\",   \"val_prefix\": \"tsp_easy\"},\n",
    "    \"MEDIUM\": {\"ckpt\": \"data_repo/MEDIUM/checkpoint_MEDIUM_best.pth\", \"val_folder\": \"Data/Validation/Medium\", \"val_prefix\": \"tsp_medium\"},\n",
    "    \"HARD\":   {\"ckpt\": \"data_repo/HARD/checkpoint_HARD_best.pth\",   \"val_folder\": \"Data/Validation/Hard\",   \"val_prefix\": \"tsp_hard\"}\n",
    "}\n",
    "\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "def plot_route(ax, points, tour, title, color):\n",
    "    \"\"\"Dibuja una ruta en el subplot dado.\"\"\"\n",
    "    # points: array numpy [N, 2]\n",
    "    # tour: array numpy [N] (indices)\n",
    "    \n",
    "    # Reordenamos los puntos seg√∫n el tour\n",
    "    route_points = points[tour]\n",
    "    # Cerramos el ciclo (a√±adimos el primer punto al final)\n",
    "    route_points = np.vstack([route_points, route_points[0]])\n",
    "    \n",
    "    # Dibujar l√≠neas\n",
    "    ax.plot(route_points[:, 0], route_points[:, 1], c=color, linewidth=1.5, linestyle='-')\n",
    "    # Dibujar nodos\n",
    "    ax.scatter(points[:, 0], points[:, 1], c='black', s=15, zorder=5)\n",
    "    # Marcar inicio (rojo)\n",
    "    ax.scatter(route_points[0, 0], route_points[0, 1], c='red', s=40, zorder=6, label='Inicio')\n",
    "    \n",
    "    ax.set_title(title)\n",
    "    ax.set_xticks([])\n",
    "    ax.set_yticks([])\n",
    "    ax.set_aspect('equal')\n",
    "\n",
    "def visualize_comparison(phase_name, config):\n",
    "    print(f\"\\nüé® GENERANDO VISUALIZACIONES PARA: {phase_name}\")\n",
    "    \n",
    "    if not os.path.exists(config[\"ckpt\"]):\n",
    "        print(f\"‚ö†Ô∏è No hay modelo para {phase_name}, saltando...\")\n",
    "        return\n",
    "\n",
    "    # 1. Cargar Modelo\n",
    "    model = EncoderPointerModel(input_dim=2, d_model=128, nhead=8, enc_layers=3, dec_layers=2, max_seq_len=150).to(DEVICE)\n",
    "    try:\n",
    "        model.load_state_dict(torch.load(config[\"ckpt\"], map_location=DEVICE, weights_only=False))\n",
    "        model.eval()\n",
    "    except Exception as e:\n",
    "        print(f\"‚ùå Error cargando modelo: {e}\")\n",
    "        return\n",
    "\n",
    "    # 2. Buscar archivos parciales\n",
    "    search_pattern = os.path.join(config[\"val_folder\"], f\"{config['val_prefix']}*.npz\")\n",
    "    files = sorted(glob.glob(search_pattern))\n",
    "    \n",
    "    if not files:\n",
    "        print(\"‚ùå No encontr√© archivos de validaci√≥n.\")\n",
    "        return\n",
    "\n",
    "    # 3. Iterar sobre cada archivo encontrado\n",
    "    print(f\"üì∏ Se encontraron {len(files)} archivos. Generando 1 ejemplo de cada uno...\")\n",
    "\n",
    "    for i, f_path in enumerate(files):\n",
    "        try:\n",
    "            # Cargar archivo\n",
    "            data = np.load(f_path, allow_pickle=True)\n",
    "            points_all = data['points']\n",
    "            sols_all = data['solutions']\n",
    "            \n",
    "            # --- SELECCIONAR UN EJEMPLO ALEATORIO O EL PRIMERO ---\n",
    "            idx = 0 # Tomamos el primero de cada archivo (puedes cambiar a np.random.randint)\n",
    "            \n",
    "            sample_points = points_all[idx] # [N, 2]\n",
    "            \n",
    "            # Fix conversi√≥n object -> int64 para la soluci√≥n real\n",
    "            raw_sol = sols_all[idx]\n",
    "            if isinstance(raw_sol, list) or raw_sol.dtype == np.object_:\n",
    "                 sample_sol_true = np.array(raw_sol).astype(np.int64)\n",
    "            else:\n",
    "                 sample_sol_true = raw_sol.astype(np.int64)\n",
    "\n",
    "            # Normalizar puntos para el modelo (0-1)\n",
    "            max_val = sample_points.max()\n",
    "            input_points = torch.tensor(sample_points / max_val, dtype=torch.float32).unsqueeze(0).to(DEVICE)\n",
    "\n",
    "            # --- INFERENCIA DEL MODELO ---\n",
    "            with torch.no_grad():\n",
    "                logits = model(input_points, teacher_forcing=False)\n",
    "                sample_sol_pred = logits.argmax(dim=2).squeeze(0).cpu().numpy()\n",
    "\n",
    "            # --- DIBUJAR ---\n",
    "            fig, axs = plt.subplots(1, 2, figsize=(10, 5))\n",
    "            \n",
    "            # Gr√°fica Izquierda: Tu IA\n",
    "            plot_route(axs[0], sample_points, sample_sol_pred, f\"Tu Modelo (IA)\\nArchivo: {os.path.basename(f_path)}\", 'blue')\n",
    "            \n",
    "            # Gr√°fica Derecha: OR-Tools (El Maestro)\n",
    "            plot_route(axs[1], sample_points, sample_sol_true, \"OR-Tools (Ground Truth)\", 'green')\n",
    "            \n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "            \n",
    "            # Limite de seguridad: Si hay 50 archivos, no queremos 50 popups.\n",
    "            # Comenta estas dos l√≠neas si quieres verlos TODOS.\n",
    "            if i >= 2: \n",
    "                print(\"üõë Deteniendo visualizaci√≥n para no saturar la pantalla (3 ejemplos mostrados).\")\n",
    "                break\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"‚ö†Ô∏è Error visualizando {os.path.basename(f_path)}: {e}\")\n",
    "            continue\n",
    "\n",
    "# --- EJECUTAR ---\n",
    "visualize_comparison(\"EASY\", PATHS_CONFIG[\"EASY\"])\n",
    "visualize_comparison(\"MEDIUM\", PATHS_CONFIG[\"MEDIUM\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
